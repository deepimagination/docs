

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>imaginaire.layers package &mdash; Imaginaire  documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/custom.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="imaginaire.losses package" href="imaginaire.losses.html" />
    <link rel="prev" title="imaginaire.generators package" href="imaginaire.generators.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html">
          

          
            
            <img src="_static/imaginaire_logo_tight.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">imaginaire_release</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="evaluate.html">evaluate module</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="imaginaire.html">imaginaire package</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="imaginaire.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="imaginaire.datasets.html">imaginaire.datasets package</a></li>
<li class="toctree-l4"><a class="reference internal" href="imaginaire.discriminators.html">imaginaire.discriminators package</a></li>
<li class="toctree-l4"><a class="reference internal" href="imaginaire.evaluation.html">imaginaire.evaluation package</a></li>
<li class="toctree-l4"><a class="reference internal" href="imaginaire.generators.html">imaginaire.generators package</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">imaginaire.layers package</a></li>
<li class="toctree-l4"><a class="reference internal" href="imaginaire.losses.html">imaginaire.losses package</a></li>
<li class="toctree-l4"><a class="reference internal" href="imaginaire.model_utils.html">imaginaire.model_utils package</a></li>
<li class="toctree-l4"><a class="reference internal" href="imaginaire.optimizers.html">imaginaire.optimizers package</a></li>
<li class="toctree-l4"><a class="reference internal" href="imaginaire.trainers.html">imaginaire.trainers package</a></li>
<li class="toctree-l4"><a class="reference internal" href="imaginaire.utils.html">imaginaire.utils package</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="imaginaire.html#submodules">Submodules</a></li>
<li class="toctree-l3"><a class="reference internal" href="imaginaire.html#module-imaginaire.config">imaginaire.config module</a></li>
<li class="toctree-l3"><a class="reference internal" href="imaginaire.html#module-imaginaire">Module contents</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="inference.html">inference module</a></li>
<li class="toctree-l2"><a class="reference internal" href="train.html">train module</a></li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Imaginaire</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="modules.html">imaginaire_release</a> &raquo;</li>
        
          <li><a href="imaginaire.html">imaginaire package</a> &raquo;</li>
        
      <li>imaginaire.layers package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/imaginaire.layers.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="imaginaire-layers-package">
<h1>imaginaire.layers package<a class="headerlink" href="#imaginaire-layers-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-imaginaire.layers.activation_norm">
<span id="imaginaire-layers-activation-norm-module"></span><h2>imaginaire.layers.activation_norm module<a class="headerlink" href="#module-imaginaire.layers.activation_norm" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="imaginaire.layers.activation_norm.AdaptiveNorm">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.activation_norm.</code><code class="sig-name descname">AdaptiveNorm</code><span class="sig-paren">(</span><em class="sig-param">num_features</em>, <em class="sig-param">cond_dims</em>, <em class="sig-param">weight_norm_type=''</em>, <em class="sig-param">projection=True</em>, <em class="sig-param">separate_projection=False</em>, <em class="sig-param">input_dim=2</em>, <em class="sig-param">activation_norm_type='instance'</em>, <em class="sig-param">activation_norm_params=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/activation_norm.html#AdaptiveNorm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.activation_norm.AdaptiveNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Adaptive normalization layer. The layer first normalizes the input, then
performs an affine transformation using parameters computed from the
conditional inputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_features</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>cond_dims</strong> (<em>int</em>) – Number of channels in the conditional inputs.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>projection</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, project the conditional input to gamma
and beta using a fully connected layer, otherwise directly use
the conditional input as gamma and beta.</p></li>
<li><p><strong>separate_projection</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, we will use two different
layers for gamma and beta. Otherwise, we will use one layer. It
matters only if you apply any weight norms to this layer.</p></li>
<li><p><strong>input_dim</strong> (<em>int</em>) – Number of dimensions of the input tensor.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="imaginaire.layers.activation_norm.AdaptiveNorm.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">y</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/activation_norm.html#AdaptiveNorm.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.activation_norm.AdaptiveNorm.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Adaptive Normalization forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>N x C1 x * tensor</em>) – Input tensor.</p></li>
<li><p><strong>y</strong> (<em>N x C2 tensor</em>) – Conditional information.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output tensor.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>out (N x C1 x * tensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="imaginaire.layers.activation_norm.AdaptiveNorm.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.activation_norm.AdaptiveNorm.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.activation_norm.HyperSpatiallyAdaptiveNorm">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.activation_norm.</code><code class="sig-name descname">HyperSpatiallyAdaptiveNorm</code><span class="sig-paren">(</span><em class="sig-param">num_features</em>, <em class="sig-param">cond_dims</em>, <em class="sig-param">num_filters=0</em>, <em class="sig-param">kernel_size=3</em>, <em class="sig-param">weight_norm_type=''</em>, <em class="sig-param">activation_norm_type='sync_batch'</em>, <em class="sig-param">is_hyper=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/activation_norm.html#HyperSpatiallyAdaptiveNorm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.activation_norm.HyperSpatiallyAdaptiveNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Spatially Adaptive Normalization (SPADE) initialization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_features</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>cond_dims</strong> (<em>int</em><em> or </em><em>list of int</em>) – List of numbers of channels
in the conditional input.</p></li>
<li><p><strong>num_filters</strong> (<em>int</em>) – Number of filters in SPADE.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em>) – <dl class="simple">
<dt>Kernel size of the convolutional filters in</dt><dd><p>the SPADE layer.</p>
</dd>
<dt>weight_norm_type (str): Type of weight normalization.</dt><dd><p><code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>.</p>
</dd>
</dl>
</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>.</p></li>
<li><p><strong>is_hyper</strong> (<em>bool</em>) – Whether to use hyper SPADE.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="imaginaire.layers.activation_norm.HyperSpatiallyAdaptiveNorm.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">*cond_inputs</em>, <em class="sig-param">norm_weights=(None</em>, <em class="sig-param">None)</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/activation_norm.html#HyperSpatiallyAdaptiveNorm.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.activation_norm.HyperSpatiallyAdaptiveNorm.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Spatially Adaptive Normalization (SPADE) forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>4D tensor</em>) – Input tensor.</p></li>
<li><p><strong>cond_inputs</strong> (<em>list of tensors</em>) – Conditional maps for SPADE.</p></li>
<li><p><strong>norm_weights</strong> (<em>5D tensor</em><em> or </em><em>list of tensors</em>) – conv weights or</p></li>
<li><p><strong>biases</strong><strong>]</strong><strong></strong> (<em>[</em><em>weights</em><em>,</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output tensor.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>output (4D tensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="imaginaire.layers.activation_norm.HyperSpatiallyAdaptiveNorm.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.activation_norm.HyperSpatiallyAdaptiveNorm.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.activation_norm.LayerNorm2d">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.activation_norm.</code><code class="sig-name descname">LayerNorm2d</code><span class="sig-paren">(</span><em class="sig-param">num_features</em>, <em class="sig-param">eps=1e-05</em>, <em class="sig-param">affine=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/activation_norm.html#LayerNorm2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.activation_norm.LayerNorm2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Layer Normalization as introduced in
<a class="reference external" href="https://arxiv.org/abs/1607.06450">https://arxiv.org/abs/1607.06450</a>.
This is the usual way to apply layer normalization in CNNs.
Note that unlike the pytorch implementation which applies per-element
scale and bias, here it applies per-channel scale and bias, similar to
batch/instance normalization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_features</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default=1e-5</em>) – a value added to the
denominator for numerical stability.</p></li>
<li><p><strong>affine</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, performs
affine transformation after normalization.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="imaginaire.layers.activation_norm.LayerNorm2d.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/activation_norm.html#LayerNorm2d.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.activation_norm.LayerNorm2d.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>tensor</em>) – Input tensor.</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="imaginaire.layers.activation_norm.LayerNorm2d.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.activation_norm.LayerNorm2d.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.activation_norm.SpatiallyAdaptiveNorm">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.activation_norm.</code><code class="sig-name descname">SpatiallyAdaptiveNorm</code><span class="sig-paren">(</span><em class="sig-param">num_features</em>, <em class="sig-param">cond_dims</em>, <em class="sig-param">num_filters=128</em>, <em class="sig-param">kernel_size=3</em>, <em class="sig-param">weight_norm_type=''</em>, <em class="sig-param">separate_projection=False</em>, <em class="sig-param">activation_norm_type='sync_batch'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">partial=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/activation_norm.html#SpatiallyAdaptiveNorm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.activation_norm.SpatiallyAdaptiveNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Spatially Adaptive Normalization (SPADE) initialization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_features</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>cond_dims</strong> (<em>int</em><em> or </em><em>list of int</em>) – List of numbers of channels
in the input.</p></li>
<li><p><strong>num_filters</strong> (<em>int</em>) – Number of filters in SPADE.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em>) – <dl class="simple">
<dt>Kernel size of the convolutional filters in</dt><dd><p>the SPADE layer.</p>
</dd>
<dt>weight_norm_type (str): Type of weight normalization.</dt><dd><p><code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>.</p>
</dd>
</dl>
</p></li>
<li><p><strong>separate_projection</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, we will use two different
layers for gamma and beta. Otherwise, we will use one layer. It
matters only if you apply any weight norms to this layer.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="imaginaire.layers.activation_norm.SpatiallyAdaptiveNorm.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">*cond_inputs</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/activation_norm.html#SpatiallyAdaptiveNorm.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.activation_norm.SpatiallyAdaptiveNorm.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Spatially Adaptive Normalization (SPADE) forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>N x C1 x H x W tensor</em>) – Input tensor.</p></li>
<li><p><strong>cond_inputs</strong> (<em>list of tensors</em>) – Conditional maps for SPADE.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output tensor.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>output (4D tensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="imaginaire.layers.activation_norm.SpatiallyAdaptiveNorm.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.activation_norm.SpatiallyAdaptiveNorm.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="imaginaire.layers.activation_norm.get_activation_norm_layer">
<code class="sig-prename descclassname">imaginaire.layers.activation_norm.</code><code class="sig-name descname">get_activation_norm_layer</code><span class="sig-paren">(</span><em class="sig-param">num_features</em>, <em class="sig-param">norm_type</em>, <em class="sig-param">input_dim</em>, <em class="sig-param">**norm_params</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/activation_norm.html#get_activation_norm_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.activation_norm.get_activation_norm_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Return an activation normalization layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_features</strong> (<em>int</em>) – Number of feature channels.</p></li>
<li><p><strong>norm_type</strong> (<em>str</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>input_dim</strong> (<em>int</em>) – Number of input dimensions.</p></li>
<li><p><strong>norm_params</strong> – Arbitrary keyword arguments that will be used to
initialize the activation normalization.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-imaginaire.layers.conv">
<span id="imaginaire-layers-conv-module"></span><h2>imaginaire.layers.conv module<a class="headerlink" href="#module-imaginaire.layers.conv" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="imaginaire.layers.conv.Conv1dBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.conv.</code><code class="sig-name descname">Conv1dBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size</em>, <em class="sig-param">stride=1</em>, <em class="sig-param">padding=0</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">padding_mode='zeros'</em>, <em class="sig-param">weight_norm_type='none'</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type='none'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">nonlinearity='none'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">order='CNA'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/conv.html#Conv1dBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.conv.Conv1dBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.conv._BaseConvBlock</span></code></p>
<p>A Wrapper class that wraps <code class="docutils literal notranslate"><span class="pre">torch.nn.Conv1d</span></code> with normalization and
nonlinearity.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple</em>) – Size of the convolving kernel.</p></li>
<li><p><strong>stride</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Stride of the convolution.</p></li>
<li><p><strong>padding</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=0</em>) – Zero-padding added to both sides of the input.</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Spacing between kernel elements.</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Number of blocked connections
from input channels to output channels.</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output.</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em><em>, </em><em>default='zeros'</em>) – Type of padding:
<code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layer.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds
Gaussian noise with learnable magnitude to the convolution output.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNA'</em>) – Order of operations.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: convolution,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.
For example, a block initialized with <code class="docutils literal notranslate"><span class="pre">order='CNA'</span></code> will
do convolution first, then normalization, then nonlinearity.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.conv.Conv1dBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.conv.Conv1dBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.conv.Conv2dBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.conv.</code><code class="sig-name descname">Conv2dBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size</em>, <em class="sig-param">stride=1</em>, <em class="sig-param">padding=0</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">padding_mode='zeros'</em>, <em class="sig-param">weight_norm_type='none'</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type='none'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">nonlinearity='none'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">order='CNA'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/conv.html#Conv2dBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.conv.Conv2dBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.conv._BaseConvBlock</span></code></p>
<p>A Wrapper class that wraps <code class="docutils literal notranslate"><span class="pre">torch.nn.Conv2d</span></code> with normalization and
nonlinearity.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple</em>) – Size of the convolving kernel.</p></li>
<li><p><strong>stride</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Stride of the convolution.</p></li>
<li><p><strong>padding</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=0</em>) – Zero-padding added to both sides of the input.</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Spacing between kernel elements.</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Number of blocked connections
from input channels to output channels.</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output.</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em><em>, </em><em>default='zeros'</em>) – Type of padding:
<code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layer.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds
Gaussian noise with learnable magnitude to the convolution output.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNA'</em>) – Order of operations.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: convolution,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.
For example, a block initialized with <code class="docutils literal notranslate"><span class="pre">order='CNA'</span></code> will
do convolution first, then normalization, then nonlinearity.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.conv.Conv2dBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.conv.Conv2dBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.conv.Conv3dBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.conv.</code><code class="sig-name descname">Conv3dBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size</em>, <em class="sig-param">stride=1</em>, <em class="sig-param">padding=0</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">padding_mode='zeros'</em>, <em class="sig-param">weight_norm_type='none'</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type='none'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">nonlinearity='none'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">order='CNA'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/conv.html#Conv3dBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.conv.Conv3dBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.conv._BaseConvBlock</span></code></p>
<p>A Wrapper class that wraps <code class="docutils literal notranslate"><span class="pre">torch.nn.Conv3d</span></code> with normalization and
nonlinearity.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple</em>) – Size of the convolving kernel.</p></li>
<li><p><strong>stride</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Stride of the convolution.</p></li>
<li><p><strong>padding</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=0</em>) – Zero-padding added to both sides of the input.</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Spacing between kernel elements.</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Number of blocked connections
from input channels to output channels.</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output.</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em><em>, </em><em>default='zeros'</em>) – Type of padding:
<code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layer.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds
Gaussian noise with learnable magnitude to the convolution output.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNA'</em>) – Order of operations.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: convolution,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.
For example, a block initialized with <code class="docutils literal notranslate"><span class="pre">order='CNA'</span></code> will
do convolution first, then normalization, then nonlinearity.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.conv.Conv3dBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.conv.Conv3dBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.conv.HyperConv2d">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.conv.</code><code class="sig-name descname">HyperConv2d</code><span class="sig-paren">(</span><em class="sig-param">in_channels=0</em>, <em class="sig-param">out_channels=0</em>, <em class="sig-param">kernel_size=3</em>, <em class="sig-param">stride=1</em>, <em class="sig-param">padding=1</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">padding_mode='zeros'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/conv.html#HyperConv2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.conv.HyperConv2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Hyper Conv2d initialization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Dummy parameter.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Dummy parameter.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple</em>) – Dummy parameter.</p></li>
<li><p><strong>stride</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Stride of the convolution. Default: 1</p></li>
<li><p><strong>padding</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=0</em>) – Zero-padding added to both sides of the input.</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em><em>, </em><em>default='zeros'</em>) – <code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Spacing between kernel elements.</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Number of blocked connections
from input channels to output channels.</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
adds a learnable bias to the output.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="imaginaire.layers.conv.HyperConv2d.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">*args</em>, <em class="sig-param">conv_weights=(None</em>, <em class="sig-param">None)</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/conv.html#HyperConv2d.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.conv.HyperConv2d.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Hyper Conv2d forward. Convolve x using the provided weight and bias.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>N x C x H x W tensor</em>) – Input tensor.</p></li>
<li><p><strong>conv_weights</strong> (<em>N x C2 x C1 x k x k tensor</em><em> or </em><em>list of tensors</em>) – Convolution weights or [weight, bias].</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output tensor.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>y (N x C2 x H x W tensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="imaginaire.layers.conv.HyperConv2d.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.conv.HyperConv2d.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.conv.HyperConv2dBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.conv.</code><code class="sig-name descname">HyperConv2dBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size</em>, <em class="sig-param">stride=1</em>, <em class="sig-param">padding=0</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">padding_mode='zeros'</em>, <em class="sig-param">weight_norm_type='none'</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type='none'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">is_hyper_conv=False</em>, <em class="sig-param">is_hyper_norm=False</em>, <em class="sig-param">nonlinearity='none'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">order='CNA'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/conv.html#HyperConv2dBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.conv.HyperConv2dBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.conv._BaseHyperConvBlock</span></code></p>
<p>A Wrapper class that wraps <code class="docutils literal notranslate"><span class="pre">HyperConv2d</span></code> with normalization and
nonlinearity.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple</em>) – Size of the convolving kernel.</p></li>
<li><p><strong>stride</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Stride of the convolution.</p></li>
<li><p><strong>padding</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=0</em>) – Zero-padding added to both sides of the input.</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Spacing between kernel elements.</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Number of blocked connections
from input channels to output channels.</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output.</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em><em>, </em><em>default='zeros'</em>) – Type of padding:
<code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>is_hyper_conv</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, use
<code class="docutils literal notranslate"><span class="pre">HyperConv2d</span></code>, otherwise use <code class="docutils literal notranslate"><span class="pre">torch.nn.Conv2d</span></code>.</p></li>
<li><p><strong>is_hyper_norm</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, use
hyper normalizations.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layer.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds
Gaussian noise with learnable magnitude to the convolution output.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNA'</em>) – Order of operations.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: convolution,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.
For example, a block initialized with <code class="docutils literal notranslate"><span class="pre">order='CNA'</span></code> will
do convolution first, then normalization, then nonlinearity.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.conv.HyperConv2dBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.conv.HyperConv2dBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.conv.LinearBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.conv.</code><code class="sig-name descname">LinearBlock</code><span class="sig-paren">(</span><em class="sig-param">in_features</em>, <em class="sig-param">out_features</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">weight_norm_type='none'</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type='none'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">nonlinearity='none'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">order='CNA'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/conv.html#LinearBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.conv.LinearBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.conv._BaseConvBlock</span></code></p>
<p>A Wrapper class that wraps <code class="docutils literal notranslate"><span class="pre">torch.nn.Linear</span></code> with normalization and
nonlinearity.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_features</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_features</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layer.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, add
Gaussian noise with learnable magnitude after the
fully-connected layer.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNA'</em>) – Order of operations.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: fully-connected,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.
For example, a block initialized with <code class="docutils literal notranslate"><span class="pre">order='CNA'</span></code> will
do convolution first, then normalization, then nonlinearity.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.conv.LinearBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.conv.LinearBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.conv.MultiOutConv2dBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.conv.</code><code class="sig-name descname">MultiOutConv2dBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size</em>, <em class="sig-param">stride=1</em>, <em class="sig-param">padding=0</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">padding_mode='zeros'</em>, <em class="sig-param">weight_norm_type='none'</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type='none'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">nonlinearity='none'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">order='CNA'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/conv.html#MultiOutConv2dBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.conv.MultiOutConv2dBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.conv._MultiOutBaseConvBlock</span></code></p>
<p>A Wrapper class that wraps <code class="docutils literal notranslate"><span class="pre">torch.nn.Conv2d</span></code> with normalization and
nonlinearity. It can return multiple outputs, if some layers in the block
return more than one output.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple</em>) – Size of the convolving kernel.</p></li>
<li><p><strong>stride</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Stride of the convolution.</p></li>
<li><p><strong>padding</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=0</em>) – Zero-padding added to both sides of the input.</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Spacing between kernel elements.</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Number of blocked connections
from input channels to output channels.</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output.</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em><em>, </em><em>default='zeros'</em>) – Type of padding:
<code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layer.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds
Gaussian noise with learnable magnitude to the convolution output.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNA'</em>) – Order of operations.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: convolution,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.
For example, a block initialized with <code class="docutils literal notranslate"><span class="pre">order='CNA'</span></code> will
do convolution first, then normalization, then nonlinearity.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.conv.MultiOutConv2dBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.conv.MultiOutConv2dBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.conv.PartialConv2d">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.conv.</code><code class="sig-name descname">PartialConv2d</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">multi_channel=False</em>, <em class="sig-param">return_mask=True</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/conv.html#PartialConv2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.conv.PartialConv2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.conv.Conv2d</span></code></p>
<p>Partial 2D convolution in
“Image inpainting for irregular holes using partial convolutions.”
Liu et al., ECCV 2018</p>
<dl class="attribute">
<dt id="imaginaire.layers.conv.PartialConv2d.bias">
<code class="sig-name descname">bias</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.conv.PartialConv2d.bias" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="imaginaire.layers.conv.PartialConv2d.dilation">
<code class="sig-name descname">dilation</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.conv.PartialConv2d.dilation" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="imaginaire.layers.conv.PartialConv2d.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">mask_in=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/conv.html#PartialConv2d.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.conv.PartialConv2d.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>tensor</em>) – Input tensor.</p></li>
<li><p><strong>mask_in</strong> (<em>tensor</em><em>, </em><em>optional</em><em>, </em><em>default=``None``</em>) – it masks the valid input region.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="imaginaire.layers.conv.PartialConv2d.groups">
<code class="sig-name descname">groups</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.conv.PartialConv2d.groups" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="imaginaire.layers.conv.PartialConv2d.kernel_size">
<code class="sig-name descname">kernel_size</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.conv.PartialConv2d.kernel_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="imaginaire.layers.conv.PartialConv2d.out_channels">
<code class="sig-name descname">out_channels</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.conv.PartialConv2d.out_channels" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="imaginaire.layers.conv.PartialConv2d.output_padding">
<code class="sig-name descname">output_padding</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.conv.PartialConv2d.output_padding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="imaginaire.layers.conv.PartialConv2d.padding">
<code class="sig-name descname">padding</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.conv.PartialConv2d.padding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="imaginaire.layers.conv.PartialConv2d.padding_mode">
<code class="sig-name descname">padding_mode</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.conv.PartialConv2d.padding_mode" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="imaginaire.layers.conv.PartialConv2d.stride">
<code class="sig-name descname">stride</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.conv.PartialConv2d.stride" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="imaginaire.layers.conv.PartialConv2d.transposed">
<code class="sig-name descname">transposed</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.conv.PartialConv2d.transposed" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="imaginaire.layers.conv.PartialConv2d.weight">
<code class="sig-name descname">weight</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.conv.PartialConv2d.weight" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.conv.PartialConv2dBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.conv.</code><code class="sig-name descname">PartialConv2dBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size</em>, <em class="sig-param">stride=1</em>, <em class="sig-param">padding=0</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">padding_mode='zeros'</em>, <em class="sig-param">weight_norm_type='none'</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type='none'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">nonlinearity='none'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">multi_channel=False</em>, <em class="sig-param">return_mask=True</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">order='CNA'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/conv.html#PartialConv2dBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.conv.PartialConv2dBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.conv._BasePartialConvBlock</span></code></p>
<p>A Wrapper class that wraps <code class="docutils literal notranslate"><span class="pre">PartialConv2d</span></code> with normalization and
nonlinearity.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple</em>) – Size of the convolving kernel.</p></li>
<li><p><strong>stride</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Stride of the convolution.</p></li>
<li><p><strong>padding</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=0</em>) – Zero-padding added to both sides of the input.</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Spacing between kernel elements.</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Number of blocked connections
from input channels to output channels.</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output.</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em><em>, </em><em>default='zeros'</em>) – Type of padding:
<code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layer.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds
Gaussian noise with learnable magnitude to the convolution output.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNA'</em>) – Order of operations.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: convolution,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.
For example, a block initialized with <code class="docutils literal notranslate"><span class="pre">order='CNA'</span></code> will
do convolution first, then normalization, then nonlinearity.</p></li>
<li><p><strong>multi_channel</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, use
different masks for different channels.</p></li>
<li><p><strong>return_mask</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the
forward call also returns a new mask.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.conv.PartialConv2dBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.conv.PartialConv2dBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.conv.PartialConv3d">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.conv.</code><code class="sig-name descname">PartialConv3d</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">multi_channel=False</em>, <em class="sig-param">return_mask=True</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/conv.html#PartialConv3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.conv.PartialConv3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.conv.Conv3d</span></code></p>
<p>Partial 3D convolution in
“Image inpainting for irregular holes using partial convolutions.”
Liu et al., ECCV 2018</p>
<dl class="attribute">
<dt id="imaginaire.layers.conv.PartialConv3d.bias">
<code class="sig-name descname">bias</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.conv.PartialConv3d.bias" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="imaginaire.layers.conv.PartialConv3d.dilation">
<code class="sig-name descname">dilation</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.conv.PartialConv3d.dilation" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="imaginaire.layers.conv.PartialConv3d.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">mask_in=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/conv.html#PartialConv3d.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.conv.PartialConv3d.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>tensor</em>) – Input tensor.</p></li>
<li><p><strong>mask_in</strong> (<em>tensor</em><em>, </em><em>optional</em><em>, </em><em>default=``None``</em>) – masks the valid input region.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="imaginaire.layers.conv.PartialConv3d.groups">
<code class="sig-name descname">groups</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.conv.PartialConv3d.groups" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="imaginaire.layers.conv.PartialConv3d.kernel_size">
<code class="sig-name descname">kernel_size</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.conv.PartialConv3d.kernel_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="imaginaire.layers.conv.PartialConv3d.out_channels">
<code class="sig-name descname">out_channels</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.conv.PartialConv3d.out_channels" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="imaginaire.layers.conv.PartialConv3d.output_padding">
<code class="sig-name descname">output_padding</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.conv.PartialConv3d.output_padding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="imaginaire.layers.conv.PartialConv3d.padding">
<code class="sig-name descname">padding</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.conv.PartialConv3d.padding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="imaginaire.layers.conv.PartialConv3d.padding_mode">
<code class="sig-name descname">padding_mode</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.conv.PartialConv3d.padding_mode" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="imaginaire.layers.conv.PartialConv3d.stride">
<code class="sig-name descname">stride</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.conv.PartialConv3d.stride" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="imaginaire.layers.conv.PartialConv3d.transposed">
<code class="sig-name descname">transposed</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.conv.PartialConv3d.transposed" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="imaginaire.layers.conv.PartialConv3d.weight">
<code class="sig-name descname">weight</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.conv.PartialConv3d.weight" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.conv.PartialConv3dBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.conv.</code><code class="sig-name descname">PartialConv3dBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size</em>, <em class="sig-param">stride=1</em>, <em class="sig-param">padding=0</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">padding_mode='zeros'</em>, <em class="sig-param">weight_norm_type='none'</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type='none'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">nonlinearity='none'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">multi_channel=False</em>, <em class="sig-param">return_mask=True</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">order='CNA'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/conv.html#PartialConv3dBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.conv.PartialConv3dBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.conv._BasePartialConvBlock</span></code></p>
<p>A Wrapper class that wraps <code class="docutils literal notranslate"><span class="pre">PartialConv3d</span></code> with normalization and
nonlinearity.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple</em>) – Size of the convolving kernel.</p></li>
<li><p><strong>stride</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Stride of the convolution.</p></li>
<li><p><strong>padding</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=0</em>) – Zero-padding added to both sides of the input.</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Spacing between kernel elements.</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Number of blocked connections
from input channels to output channels.</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output.</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em><em>, </em><em>default='zeros'</em>) – Type of padding:
<code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layer.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds
Gaussian noise with learnable magnitude to the convolution output.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNA'</em>) – Order of operations.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: convolution,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.
For example, a block initialized with <code class="docutils literal notranslate"><span class="pre">order='CNA'</span></code> will
do convolution first, then normalization, then nonlinearity.</p></li>
<li><p><strong>multi_channel</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, use
different masks for different channels.</p></li>
<li><p><strong>return_mask</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the
forward call also returns a new mask.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.conv.PartialConv3dBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.conv.PartialConv3dBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-imaginaire.layers.misc">
<span id="imaginaire-layers-misc-module"></span><h2>imaginaire.layers.misc module<a class="headerlink" href="#module-imaginaire.layers.misc" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="imaginaire.layers.misc.ApplyNoise">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.misc.</code><code class="sig-name descname">ApplyNoise</code><a class="reference internal" href="_modules/imaginaire/layers/misc.html#ApplyNoise"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.misc.ApplyNoise" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Add Gaussian noise to the input tensor.</p>
<dl class="method">
<dt id="imaginaire.layers.misc.ApplyNoise.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">noise=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/misc.html#ApplyNoise.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.misc.ApplyNoise.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>tensor</em>) – Input tensor.</p></li>
<li><p><strong>noise</strong> (<em>tensor</em><em>, </em><em>optional</em><em>, </em><em>default=``None``</em>) – Noise tensor to be
added to the input.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="imaginaire.layers.misc.ApplyNoise.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.misc.ApplyNoise.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.misc.PartialSequential">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.misc.</code><code class="sig-name descname">PartialSequential</code><span class="sig-paren">(</span><em class="sig-param">*modules</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/misc.html#PartialSequential"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.misc.PartialSequential" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.container.Sequential</span></code></p>
<p>Sequential block for partial convolutions.</p>
<dl class="method">
<dt id="imaginaire.layers.misc.PartialSequential.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/misc.html#PartialSequential.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.misc.PartialSequential.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>tensor</em>) – Input tensor.</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="imaginaire.layers.misc.PartialSequential.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.misc.PartialSequential.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-imaginaire.layers.non_local">
<span id="imaginaire-layers-non-local-module"></span><h2>imaginaire.layers.non_local module<a class="headerlink" href="#module-imaginaire.layers.non_local" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="imaginaire.layers.non_local.NonLocal2dBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.non_local.</code><code class="sig-name descname">NonLocal2dBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">scale=True</em>, <em class="sig-param">clamp=False</em>, <em class="sig-param">weight_norm_type='none'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/non_local.html#NonLocal2dBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.non_local.NonLocal2dBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Self attention Layer</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>scale</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, scale the
output by a learnable parameter.</p></li>
<li><p><strong>clamp</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=``False``</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, clamp the
scaling parameter to (-1, 1).</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="imaginaire.layers.non_local.NonLocal2dBlock.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/non_local.html#NonLocal2dBlock.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.non_local.NonLocal2dBlock.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>tensor</em>) – input feature maps (B X C X W X H)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p>out (tensor) : self attention value + input feature</p></li>
<li><p>attention (tensor): B x N x N (N is Width*Height)</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(tuple)</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="imaginaire.layers.non_local.NonLocal2dBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.non_local.NonLocal2dBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-imaginaire.layers.nonlinearity">
<span id="imaginaire-layers-nonlinearity-module"></span><h2>imaginaire.layers.nonlinearity module<a class="headerlink" href="#module-imaginaire.layers.nonlinearity" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="imaginaire.layers.nonlinearity.get_nonlinearity_layer">
<code class="sig-prename descclassname">imaginaire.layers.nonlinearity.</code><code class="sig-name descname">get_nonlinearity_layer</code><span class="sig-paren">(</span><em class="sig-param">nonlinearity_type</em>, <em class="sig-param">inplace</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/nonlinearity.html#get_nonlinearity_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.nonlinearity.get_nonlinearity_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a nonlinearity layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>nonlinearity_type</strong> (<em>str</em>) – Type of nonlinear activation function.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing
the nonlinearity layer.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-imaginaire.layers.residual">
<span id="imaginaire-layers-residual-module"></span><h2>imaginaire.layers.residual module<a class="headerlink" href="#module-imaginaire.layers.residual" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="imaginaire.layers.residual.DownRes2dBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.residual.</code><code class="sig-name descname">DownRes2dBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size=3</em>, <em class="sig-param">padding=1</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">padding_mode='zeros'</em>, <em class="sig-param">weight_norm_type='none'</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type='none'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">skip_activation_norm=True</em>, <em class="sig-param">skip_nonlinearity=False</em>, <em class="sig-param">nonlinearity='leakyrelu'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">hidden_channels_equal_out_channels=False</em>, <em class="sig-param">order='CNACNA'</em>, <em class="sig-param">pooling=&lt;class 'torch.nn.modules.pooling.AvgPool2d'&gt;</em>, <em class="sig-param">down_factor=2</em>, <em class="sig-param">learn_shortcut=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/residual.html#DownRes2dBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.residual.DownRes2dBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.residual._BaseDownResBlock</span></code></p>
<p>Residual block for 2D input with downsampling.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=3</em>) – Kernel size for the
convolutional filters in the residual link.</p></li>
<li><p><strong>padding</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Padding size.</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Dilation factor.</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Number of convolutional/linear
groups.</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em><em>, </em><em>default='zeros'</em>) – Type of padding:
<code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>skip_activation_norm</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies activation norm to the
learned shortcut connection.</p></li>
<li><p><strong>skip_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies nonlinearity to the
learned shortcut connection.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function in the residual link.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layers.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds
Gaussian noise with learnable magnitude to the convolution output.</p></li>
<li><p><strong>hidden_channels_equal_out_channels</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, set the hidden channel number to be equal to the
output channel number. If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the hidden channel number
equals to the smaller of the input channel number and the
output channel number.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNACNA'</em>) – Order of operations
in the residual link.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: convolution,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.</p></li>
<li><p><strong>pooling</strong> (<em>class</em><em>, </em><em>optional</em><em>, </em><em>default=nn.AvgPool2d</em>) – Pytorch pooling
layer to be used.</p></li>
<li><p><strong>down_factor</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=2</em>) – Downsampling factor.</p></li>
<li><p><strong>learn_shortcut</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, always use
a convolutional shortcut instead of an identity one, otherwise only
use a convolutional one if input and output have different number of
channels.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.residual.DownRes2dBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.residual.DownRes2dBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.residual.HyperRes2dBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.residual.</code><code class="sig-name descname">HyperRes2dBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size=3</em>, <em class="sig-param">padding=1</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">padding_mode='zeros'</em>, <em class="sig-param">weight_norm_type=''</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type=''</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">skip_activation_norm=True</em>, <em class="sig-param">skip_nonlinearity=False</em>, <em class="sig-param">nonlinearity='leakyrelu'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">hidden_channels_equal_out_channels=False</em>, <em class="sig-param">order='CNACNA'</em>, <em class="sig-param">is_hyper_conv=False</em>, <em class="sig-param">is_hyper_norm=False</em>, <em class="sig-param">learn_shortcut=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/residual.html#HyperRes2dBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.residual.HyperRes2dBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.residual._BaseHyperResBlock</span></code></p>
<p>Hyper residual block for 2D input.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=3</em>) – Kernel size for the
convolutional filters in the residual link.</p></li>
<li><p><strong>padding</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Padding size.</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Dilation factor.</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Number of convolutional/linear
groups.</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em><em>, </em><em>default='zeros'</em>) – Type of padding:
<code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>skip_activation_norm</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies activation norm to the
learned shortcut connection.</p></li>
<li><p><strong>skip_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies nonlinearity to the
learned shortcut connection.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function in the residual link.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layers.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds
Gaussian noise with learnable magnitude to the convolution output.</p></li>
<li><p><strong>hidden_channels_equal_out_channels</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, set the hidden channel number to be equal to the
output channel number. If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the hidden channel number
equals to the smaller of the input channel number and the
output channel number.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNACNA'</em>) – Order of operations
in the residual link.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: convolution,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.</p></li>
<li><p><strong>is_hyper_conv</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, use
<code class="docutils literal notranslate"><span class="pre">HyperConv2d</span></code>, otherwise use <code class="docutils literal notranslate"><span class="pre">torch.nn.Conv2d</span></code>.</p></li>
<li><p><strong>is_hyper_norm</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, use
hyper normalizations.</p></li>
<li><p><strong>learn_shortcut</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, always use
a convolutional shortcut instead of an identity one, otherwise only
use a convolutional one if input and output have different number of
channels.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.residual.HyperRes2dBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.residual.HyperRes2dBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.residual.MultiOutRes2dBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.residual.</code><code class="sig-name descname">MultiOutRes2dBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size=3</em>, <em class="sig-param">padding=1</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">padding_mode='zeros'</em>, <em class="sig-param">weight_norm_type='none'</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type='none'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">skip_activation_norm=True</em>, <em class="sig-param">skip_nonlinearity=False</em>, <em class="sig-param">nonlinearity='leakyrelu'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">hidden_channels_equal_out_channels=False</em>, <em class="sig-param">order='CNACNA'</em>, <em class="sig-param">learn_shortcut=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/residual.html#MultiOutRes2dBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.residual.MultiOutRes2dBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.residual._BaseMultiOutResBlock</span></code></p>
<p>Residual block for 2D input. It can return multiple outputs, if some
layers in the block return more than one output.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=3</em>) – Kernel size for the
convolutional filters in the residual link.</p></li>
<li><p><strong>padding</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Padding size.</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Dilation factor.</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Number of convolutional/linear
groups.</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em><em>, </em><em>default='zeros'</em>) – Type of padding:
<code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>skip_activation_norm</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies activation norm to the
learned shortcut connection.</p></li>
<li><p><strong>skip_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies nonlinearity to the
learned shortcut connection.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function in the residual link.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layers.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds
Gaussian noise with learnable magnitude to the convolution output.</p></li>
<li><p><strong>hidden_channels_equal_out_channels</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, set the hidden channel number to be equal to the
output channel number. If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the hidden channel number
equals to the smaller of the input channel number and the
output channel number.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNACNA'</em>) – Order of operations
in the residual link.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: convolution,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.</p></li>
<li><p><strong>learn_shortcut</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, always use
a convolutional shortcut instead of an identity one, otherwise only
use a convolutional one if input and output have different number of
channels.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.residual.MultiOutRes2dBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.residual.MultiOutRes2dBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.residual.PartialRes2dBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.residual.</code><code class="sig-name descname">PartialRes2dBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size=3</em>, <em class="sig-param">padding=1</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">padding_mode='zeros'</em>, <em class="sig-param">weight_norm_type='none'</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type='none'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">skip_activation_norm=True</em>, <em class="sig-param">skip_nonlinearity=False</em>, <em class="sig-param">nonlinearity='leakyrelu'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">multi_channel=False</em>, <em class="sig-param">return_mask=True</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">hidden_channels_equal_out_channels=False</em>, <em class="sig-param">order='CNACNA'</em>, <em class="sig-param">learn_shortcut=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/residual.html#PartialRes2dBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.residual.PartialRes2dBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.residual._BasePartialResBlock</span></code></p>
<p>Residual block for 2D input with partial convolution.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=3</em>) – Kernel size for the
convolutional filters in the residual link.</p></li>
<li><p><strong>padding</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Padding size.</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Dilation factor.</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Number of convolutional/linear
groups.</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em><em>, </em><em>default='zeros'</em>) – Type of padding:
<code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>skip_activation_norm</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies activation norm to the
learned shortcut connection.</p></li>
<li><p><strong>skip_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies nonlinearity to the
learned shortcut connection.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function in the residual link.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layers.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds
Gaussian noise with learnable magnitude to the convolution output.</p></li>
<li><p><strong>hidden_channels_equal_out_channels</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, set the hidden channel number to be equal to the
output channel number. If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the hidden channel number
equals to the smaller of the input channel number and the
output channel number.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNACNA'</em>) – Order of operations
in the residual link.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: convolution,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.</p></li>
<li><p><strong>learn_shortcut</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, always use
a convolutional shortcut instead of an identity one, otherwise only
use a convolutional one if input and output have different number of
channels.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.residual.PartialRes2dBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.residual.PartialRes2dBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.residual.PartialRes3dBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.residual.</code><code class="sig-name descname">PartialRes3dBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size=3</em>, <em class="sig-param">padding=1</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">padding_mode='zeros'</em>, <em class="sig-param">weight_norm_type='none'</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type='none'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">skip_activation_norm=True</em>, <em class="sig-param">skip_nonlinearity=False</em>, <em class="sig-param">nonlinearity='leakyrelu'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">multi_channel=False</em>, <em class="sig-param">return_mask=True</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">hidden_channels_equal_out_channels=False</em>, <em class="sig-param">order='CNACNA'</em>, <em class="sig-param">learn_shortcut=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/residual.html#PartialRes3dBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.residual.PartialRes3dBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.residual._BasePartialResBlock</span></code></p>
<p>Residual block for 3D input with partial convolution.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=3</em>) – Kernel size for the
convolutional filters in the residual link.</p></li>
<li><p><strong>padding</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Padding size.</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Dilation factor.</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Number of convolutional/linear
groups.</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em><em>, </em><em>default='zeros'</em>) – Type of padding:
<code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>skip_activation_norm</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies activation norm to the
learned shortcut connection.</p></li>
<li><p><strong>skip_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies nonlinearity to the
learned shortcut connection.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function in the residual link.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layers.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds
Gaussian noise with learnable magnitude to the convolution output.</p></li>
<li><p><strong>hidden_channels_equal_out_channels</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, set the hidden channel number to be equal to the
output channel number. If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the hidden channel number
equals to the smaller of the input channel number and the
output channel number.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNACNA'</em>) – Order of operations
in the residual link.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: convolution,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.</p></li>
<li><p><strong>learn_shortcut</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, always use
a convolutional shortcut instead of an identity one, otherwise only
use a convolutional one if input and output have different number of
channels.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.residual.PartialRes3dBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.residual.PartialRes3dBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.residual.Res1dBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.residual.</code><code class="sig-name descname">Res1dBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size=3</em>, <em class="sig-param">padding=1</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">padding_mode='zeros'</em>, <em class="sig-param">weight_norm_type='none'</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type='none'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">skip_activation_norm=True</em>, <em class="sig-param">skip_nonlinearity=False</em>, <em class="sig-param">nonlinearity='leakyrelu'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">hidden_channels_equal_out_channels=False</em>, <em class="sig-param">order='CNACNA'</em>, <em class="sig-param">learn_shortcut=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/residual.html#Res1dBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.residual.Res1dBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.residual._BaseResBlock</span></code></p>
<p>Residual block for 1D input.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=3</em>) – Kernel size for the
convolutional filters in the residual link.</p></li>
<li><p><strong>padding</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Padding size.</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Dilation factor.</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Number of convolutional/linear
groups.</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em><em>, </em><em>default='zeros'</em>) – Type of padding:
<code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>skip_activation_norm</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies activation norm to the
learned shortcut connection.</p></li>
<li><p><strong>skip_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies nonlinearity to the
learned shortcut connection.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function in the residual link.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layers.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds
Gaussian noise with learnable magnitude to the convolution output.</p></li>
<li><p><strong>hidden_channels_equal_out_channels</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, set the hidden channel number to be equal to the
output channel number. If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the hidden channel number
equals to the smaller of the input channel number and the
output channel number.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNACNA'</em>) – Order of operations
in the residual link.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: convolution,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.</p></li>
<li><p><strong>learn_shortcut</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, always use
a convolutional shortcut instead of an identity one, otherwise only
use a convolutional one if input and output have different number of
channels.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.residual.Res1dBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.residual.Res1dBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.residual.Res2dBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.residual.</code><code class="sig-name descname">Res2dBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size=3</em>, <em class="sig-param">padding=1</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">padding_mode='zeros'</em>, <em class="sig-param">weight_norm_type='none'</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type='none'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">skip_activation_norm=True</em>, <em class="sig-param">skip_nonlinearity=False</em>, <em class="sig-param">nonlinearity='leakyrelu'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">hidden_channels_equal_out_channels=False</em>, <em class="sig-param">order='CNACNA'</em>, <em class="sig-param">learn_shortcut=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/residual.html#Res2dBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.residual.Res2dBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.residual._BaseResBlock</span></code></p>
<p>Residual block for 2D input.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=3</em>) – Kernel size for the
convolutional filters in the residual link.</p></li>
<li><p><strong>padding</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Padding size.</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Dilation factor.</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Number of convolutional/linear
groups.</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em><em>, </em><em>default='zeros'</em>) – Type of padding:
<code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>skip_activation_norm</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies activation norm to the
learned shortcut connection.</p></li>
<li><p><strong>skip_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies nonlinearity to the
learned shortcut connection.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function in the residual link.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layers.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds
Gaussian noise with learnable magnitude to the convolution output.</p></li>
<li><p><strong>hidden_channels_equal_out_channels</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, set the hidden channel number to be equal to the
output channel number. If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the hidden channel number
equals to the smaller of the input channel number and the
output channel number.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNACNA'</em>) – Order of operations
in the residual link.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: convolution,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.</p></li>
<li><p><strong>learn_shortcut</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, always use
a convolutional shortcut instead of an identity one, otherwise only
use a convolutional one if input and output have different number of
channels.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.residual.Res2dBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.residual.Res2dBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.residual.Res3dBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.residual.</code><code class="sig-name descname">Res3dBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size=3</em>, <em class="sig-param">padding=1</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">padding_mode='zeros'</em>, <em class="sig-param">weight_norm_type='none'</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type='none'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">skip_activation_norm=True</em>, <em class="sig-param">skip_nonlinearity=False</em>, <em class="sig-param">nonlinearity='leakyrelu'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">hidden_channels_equal_out_channels=False</em>, <em class="sig-param">order='CNACNA'</em>, <em class="sig-param">learn_shortcut=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/residual.html#Res3dBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.residual.Res3dBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.residual._BaseResBlock</span></code></p>
<p>Residual block for 3D input.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=3</em>) – Kernel size for the
convolutional filters in the residual link.</p></li>
<li><p><strong>padding</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Padding size.</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Dilation factor.</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Number of convolutional/linear
groups.</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em><em>, </em><em>default='zeros'</em>) – Type of padding:
<code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>skip_activation_norm</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies activation norm to the
learned shortcut connection.</p></li>
<li><p><strong>skip_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies nonlinearity to the
learned shortcut connection.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function in the residual link.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layers.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds
Gaussian noise with learnable magnitude to the convolution output.</p></li>
<li><p><strong>hidden_channels_equal_out_channels</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, set the hidden channel number to be equal to the
output channel number. If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the hidden channel number
equals to the smaller of the input channel number and the
output channel number.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNACNA'</em>) – Order of operations
in the residual link.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: convolution,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.</p></li>
<li><p><strong>learn_shortcut</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, always use
a convolutional shortcut instead of an identity one, otherwise only
use a convolutional one if input and output have different number of
channels.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.residual.Res3dBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.residual.Res3dBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.residual.ResLinearBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.residual.</code><code class="sig-name descname">ResLinearBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">weight_norm_type='none'</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type='none'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">skip_activation_norm=True</em>, <em class="sig-param">skip_nonlinearity=False</em>, <em class="sig-param">nonlinearity='leakyrelu'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">hidden_channels_equal_out_channels=False</em>, <em class="sig-param">order='CNACNA'</em>, <em class="sig-param">learn_shortcut=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/residual.html#ResLinearBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.residual.ResLinearBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.residual._BaseResBlock</span></code></p>
<p>Residual block with full-connected layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>skip_activation_norm</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies activation norm to the
learned shortcut connection.</p></li>
<li><p><strong>skip_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies nonlinearity to the
learned shortcut connection.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function in the residual link.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layers.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, add
Gaussian noise with learnable magnitude after the
fully-connected layer.</p></li>
<li><p><strong>hidden_channels_equal_out_channels</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, set the hidden channel number to be equal to the
output channel number. If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the hidden channel number
equals to the smaller of the input channel number and the
output channel number.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNACNA'</em>) – Order of operations
in the residual link.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: fully-connected,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.</p></li>
<li><p><strong>learn_shortcut</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, always use
a convolutional shortcut instead of an identity one, otherwise only
use a convolutional one if input and output have different number of
channels.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.residual.ResLinearBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.residual.ResLinearBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.residual.UpRes2dBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.residual.</code><code class="sig-name descname">UpRes2dBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size=3</em>, <em class="sig-param">padding=1</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">padding_mode='zeros'</em>, <em class="sig-param">weight_norm_type='none'</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type='none'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">skip_activation_norm=True</em>, <em class="sig-param">skip_nonlinearity=False</em>, <em class="sig-param">nonlinearity='leakyrelu'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">hidden_channels_equal_out_channels=False</em>, <em class="sig-param">order='CNACNA'</em>, <em class="sig-param">upsample=&lt;class 'torch.nn.modules.upsampling.Upsample'&gt;</em>, <em class="sig-param">up_factor=2</em>, <em class="sig-param">learn_shortcut=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/residual.html#UpRes2dBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.residual.UpRes2dBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.residual._BaseUpResBlock</span></code></p>
<p>Residual block for 2D input with downsampling.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=3</em>) – Kernel size for the
convolutional filters in the residual link.</p></li>
<li><p><strong>padding</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Padding size.</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Dilation factor.</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Number of convolutional/linear
groups.</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em><em>, </em><em>default='zeros'</em>) – Type of padding:
<code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>skip_activation_norm</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies activation norm to the
learned shortcut connection.</p></li>
<li><p><strong>skip_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies nonlinearity to the
learned shortcut connection.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function in the residual link.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layers.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds
Gaussian noise with learnable magnitude to the convolution output.</p></li>
<li><p><strong>hidden_channels_equal_out_channels</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, set the hidden channel number to be equal to the
output channel number. If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the hidden channel number
equals to the smaller of the input channel number and the
output channel number.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNACNA'</em>) – Order of operations
in the residual link.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: convolution,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.</p></li>
<li><p><strong>upsample</strong> (<em>class</em><em>, </em><em>optional</em><em>, </em><em>default=NearestUpsample</em>) – PPytorch
upsampling layer to be used.</p></li>
<li><p><strong>up_factor</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=2</em>) – Upsampling factor.</p></li>
<li><p><strong>learn_shortcut</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, always use
a convolutional shortcut instead of an identity one, otherwise only
use a convolutional one if input and output have different number of
channels.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.residual.UpRes2dBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.residual.UpRes2dBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-imaginaire.layers.weight_norm">
<span id="imaginaire-layers-weight-norm-module"></span><h2>imaginaire.layers.weight_norm module<a class="headerlink" href="#module-imaginaire.layers.weight_norm" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="imaginaire.layers.weight_norm.WeightDemodulation">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.weight_norm.</code><code class="sig-name descname">WeightDemodulation</code><span class="sig-paren">(</span><em class="sig-param">conv</em>, <em class="sig-param">cond_dims</em>, <em class="sig-param">eps=1e-08</em>, <em class="sig-param">adaptive_bias=False</em>, <em class="sig-param">demod=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/weight_norm.html#WeightDemodulation"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.weight_norm.WeightDemodulation" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Weight demodulation in
“Analyzing and Improving the Image Quality of StyleGAN”, Karras et al.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>conv</strong> (<em>torch.nn.Modules</em>) – Convolutional layer.</p></li>
<li><p><strong>cond_dims</strong> (<em>int</em>) – The number of channels in the conditional input.</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default=1e-8</em>) – a value added to the
denominator for numerical stability.</p></li>
<li><p><strong>adaptive_bias</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adaptively
predicts bias from the conditional input.</p></li>
<li><p><strong>demod</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, performs
weight demodulation.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="imaginaire.layers.weight_norm.WeightDemodulation.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">y</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/weight_norm.html#WeightDemodulation.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.weight_norm.WeightDemodulation.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Weight demodulation forward</p>
</dd></dl>

<dl class="attribute">
<dt id="imaginaire.layers.weight_norm.WeightDemodulation.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.weight_norm.WeightDemodulation.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="imaginaire.layers.weight_norm.get_weight_norm_layer">
<code class="sig-prename descclassname">imaginaire.layers.weight_norm.</code><code class="sig-name descname">get_weight_norm_layer</code><span class="sig-paren">(</span><em class="sig-param">norm_type</em>, <em class="sig-param">**norm_params</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/weight_norm.html#get_weight_norm_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.weight_norm.get_weight_norm_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Return weight normalization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>norm_type</strong> (<em>str</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>norm_params</strong> – Arbitrary keyword arguments that will be used to
initialize the weight normalization.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="imaginaire.layers.weight_norm.weight_demod">
<code class="sig-prename descclassname">imaginaire.layers.weight_norm.</code><code class="sig-name descname">weight_demod</code><span class="sig-paren">(</span><em class="sig-param">conv</em>, <em class="sig-param">cond_dims=256</em>, <em class="sig-param">eps=1e-08</em>, <em class="sig-param">demod=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/weight_norm.html#weight_demod"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.weight_norm.weight_demod" title="Permalink to this definition">¶</a></dt>
<dd><p>Weight demodulation.</p>
</dd></dl>

</div>
<div class="section" id="module-imaginaire.layers">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-imaginaire.layers" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="imaginaire.layers.Conv1dBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.</code><code class="sig-name descname">Conv1dBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size</em>, <em class="sig-param">stride=1</em>, <em class="sig-param">padding=0</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">padding_mode='zeros'</em>, <em class="sig-param">weight_norm_type='none'</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type='none'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">nonlinearity='none'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">order='CNA'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/conv.html#Conv1dBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.Conv1dBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.conv._BaseConvBlock</span></code></p>
<p>A Wrapper class that wraps <code class="docutils literal notranslate"><span class="pre">torch.nn.Conv1d</span></code> with normalization and
nonlinearity.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple</em>) – Size of the convolving kernel.</p></li>
<li><p><strong>stride</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Stride of the convolution.</p></li>
<li><p><strong>padding</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=0</em>) – Zero-padding added to both sides of the input.</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Spacing between kernel elements.</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Number of blocked connections
from input channels to output channels.</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output.</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em><em>, </em><em>default='zeros'</em>) – Type of padding:
<code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layer.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds
Gaussian noise with learnable magnitude to the convolution output.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNA'</em>) – Order of operations.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: convolution,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.
For example, a block initialized with <code class="docutils literal notranslate"><span class="pre">order='CNA'</span></code> will
do convolution first, then normalization, then nonlinearity.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.Conv1dBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.Conv1dBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.Conv2dBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.</code><code class="sig-name descname">Conv2dBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size</em>, <em class="sig-param">stride=1</em>, <em class="sig-param">padding=0</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">padding_mode='zeros'</em>, <em class="sig-param">weight_norm_type='none'</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type='none'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">nonlinearity='none'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">order='CNA'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/conv.html#Conv2dBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.Conv2dBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.conv._BaseConvBlock</span></code></p>
<p>A Wrapper class that wraps <code class="docutils literal notranslate"><span class="pre">torch.nn.Conv2d</span></code> with normalization and
nonlinearity.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple</em>) – Size of the convolving kernel.</p></li>
<li><p><strong>stride</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Stride of the convolution.</p></li>
<li><p><strong>padding</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=0</em>) – Zero-padding added to both sides of the input.</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Spacing between kernel elements.</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Number of blocked connections
from input channels to output channels.</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output.</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em><em>, </em><em>default='zeros'</em>) – Type of padding:
<code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layer.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds
Gaussian noise with learnable magnitude to the convolution output.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNA'</em>) – Order of operations.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: convolution,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.
For example, a block initialized with <code class="docutils literal notranslate"><span class="pre">order='CNA'</span></code> will
do convolution first, then normalization, then nonlinearity.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.Conv2dBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.Conv2dBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.Conv3dBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.</code><code class="sig-name descname">Conv3dBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size</em>, <em class="sig-param">stride=1</em>, <em class="sig-param">padding=0</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">padding_mode='zeros'</em>, <em class="sig-param">weight_norm_type='none'</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type='none'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">nonlinearity='none'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">order='CNA'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/conv.html#Conv3dBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.Conv3dBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.conv._BaseConvBlock</span></code></p>
<p>A Wrapper class that wraps <code class="docutils literal notranslate"><span class="pre">torch.nn.Conv3d</span></code> with normalization and
nonlinearity.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple</em>) – Size of the convolving kernel.</p></li>
<li><p><strong>stride</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Stride of the convolution.</p></li>
<li><p><strong>padding</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=0</em>) – Zero-padding added to both sides of the input.</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Spacing between kernel elements.</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Number of blocked connections
from input channels to output channels.</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output.</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em><em>, </em><em>default='zeros'</em>) – Type of padding:
<code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layer.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds
Gaussian noise with learnable magnitude to the convolution output.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNA'</em>) – Order of operations.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: convolution,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.
For example, a block initialized with <code class="docutils literal notranslate"><span class="pre">order='CNA'</span></code> will
do convolution first, then normalization, then nonlinearity.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.Conv3dBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.Conv3dBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.LinearBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.</code><code class="sig-name descname">LinearBlock</code><span class="sig-paren">(</span><em class="sig-param">in_features</em>, <em class="sig-param">out_features</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">weight_norm_type='none'</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type='none'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">nonlinearity='none'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">order='CNA'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/conv.html#LinearBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.LinearBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.conv._BaseConvBlock</span></code></p>
<p>A Wrapper class that wraps <code class="docutils literal notranslate"><span class="pre">torch.nn.Linear</span></code> with normalization and
nonlinearity.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_features</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_features</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layer.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, add
Gaussian noise with learnable magnitude after the
fully-connected layer.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNA'</em>) – Order of operations.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: fully-connected,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.
For example, a block initialized with <code class="docutils literal notranslate"><span class="pre">order='CNA'</span></code> will
do convolution first, then normalization, then nonlinearity.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.LinearBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.LinearBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.HyperConv2dBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.</code><code class="sig-name descname">HyperConv2dBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size</em>, <em class="sig-param">stride=1</em>, <em class="sig-param">padding=0</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">padding_mode='zeros'</em>, <em class="sig-param">weight_norm_type='none'</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type='none'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">is_hyper_conv=False</em>, <em class="sig-param">is_hyper_norm=False</em>, <em class="sig-param">nonlinearity='none'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">order='CNA'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/conv.html#HyperConv2dBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.HyperConv2dBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.conv._BaseHyperConvBlock</span></code></p>
<p>A Wrapper class that wraps <code class="docutils literal notranslate"><span class="pre">HyperConv2d</span></code> with normalization and
nonlinearity.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple</em>) – Size of the convolving kernel.</p></li>
<li><p><strong>stride</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Stride of the convolution.</p></li>
<li><p><strong>padding</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=0</em>) – Zero-padding added to both sides of the input.</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Spacing between kernel elements.</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Number of blocked connections
from input channels to output channels.</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output.</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em><em>, </em><em>default='zeros'</em>) – Type of padding:
<code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>is_hyper_conv</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, use
<code class="docutils literal notranslate"><span class="pre">HyperConv2d</span></code>, otherwise use <code class="docutils literal notranslate"><span class="pre">torch.nn.Conv2d</span></code>.</p></li>
<li><p><strong>is_hyper_norm</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, use
hyper normalizations.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layer.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds
Gaussian noise with learnable magnitude to the convolution output.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNA'</em>) – Order of operations.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: convolution,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.
For example, a block initialized with <code class="docutils literal notranslate"><span class="pre">order='CNA'</span></code> will
do convolution first, then normalization, then nonlinearity.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.HyperConv2dBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.HyperConv2dBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.MultiOutConv2dBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.</code><code class="sig-name descname">MultiOutConv2dBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size</em>, <em class="sig-param">stride=1</em>, <em class="sig-param">padding=0</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">padding_mode='zeros'</em>, <em class="sig-param">weight_norm_type='none'</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type='none'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">nonlinearity='none'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">order='CNA'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/conv.html#MultiOutConv2dBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.MultiOutConv2dBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.conv._MultiOutBaseConvBlock</span></code></p>
<p>A Wrapper class that wraps <code class="docutils literal notranslate"><span class="pre">torch.nn.Conv2d</span></code> with normalization and
nonlinearity. It can return multiple outputs, if some layers in the block
return more than one output.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple</em>) – Size of the convolving kernel.</p></li>
<li><p><strong>stride</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Stride of the convolution.</p></li>
<li><p><strong>padding</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=0</em>) – Zero-padding added to both sides of the input.</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Spacing between kernel elements.</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Number of blocked connections
from input channels to output channels.</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output.</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em><em>, </em><em>default='zeros'</em>) – Type of padding:
<code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layer.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds
Gaussian noise with learnable magnitude to the convolution output.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNA'</em>) – Order of operations.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: convolution,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.
For example, a block initialized with <code class="docutils literal notranslate"><span class="pre">order='CNA'</span></code> will
do convolution first, then normalization, then nonlinearity.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.MultiOutConv2dBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.MultiOutConv2dBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.PartialConv2dBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.</code><code class="sig-name descname">PartialConv2dBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size</em>, <em class="sig-param">stride=1</em>, <em class="sig-param">padding=0</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">padding_mode='zeros'</em>, <em class="sig-param">weight_norm_type='none'</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type='none'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">nonlinearity='none'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">multi_channel=False</em>, <em class="sig-param">return_mask=True</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">order='CNA'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/conv.html#PartialConv2dBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.PartialConv2dBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.conv._BasePartialConvBlock</span></code></p>
<p>A Wrapper class that wraps <code class="docutils literal notranslate"><span class="pre">PartialConv2d</span></code> with normalization and
nonlinearity.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple</em>) – Size of the convolving kernel.</p></li>
<li><p><strong>stride</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Stride of the convolution.</p></li>
<li><p><strong>padding</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=0</em>) – Zero-padding added to both sides of the input.</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Spacing between kernel elements.</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Number of blocked connections
from input channels to output channels.</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output.</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em><em>, </em><em>default='zeros'</em>) – Type of padding:
<code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layer.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds
Gaussian noise with learnable magnitude to the convolution output.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNA'</em>) – Order of operations.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: convolution,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.
For example, a block initialized with <code class="docutils literal notranslate"><span class="pre">order='CNA'</span></code> will
do convolution first, then normalization, then nonlinearity.</p></li>
<li><p><strong>multi_channel</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, use
different masks for different channels.</p></li>
<li><p><strong>return_mask</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the
forward call also returns a new mask.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.PartialConv2dBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.PartialConv2dBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.PartialConv3dBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.</code><code class="sig-name descname">PartialConv3dBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size</em>, <em class="sig-param">stride=1</em>, <em class="sig-param">padding=0</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">padding_mode='zeros'</em>, <em class="sig-param">weight_norm_type='none'</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type='none'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">nonlinearity='none'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">multi_channel=False</em>, <em class="sig-param">return_mask=True</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">order='CNA'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/conv.html#PartialConv3dBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.PartialConv3dBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.conv._BasePartialConvBlock</span></code></p>
<p>A Wrapper class that wraps <code class="docutils literal notranslate"><span class="pre">PartialConv3d</span></code> with normalization and
nonlinearity.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple</em>) – Size of the convolving kernel.</p></li>
<li><p><strong>stride</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Stride of the convolution.</p></li>
<li><p><strong>padding</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=0</em>) – Zero-padding added to both sides of the input.</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Spacing between kernel elements.</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Number of blocked connections
from input channels to output channels.</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output.</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em><em>, </em><em>default='zeros'</em>) – Type of padding:
<code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layer.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds
Gaussian noise with learnable magnitude to the convolution output.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNA'</em>) – Order of operations.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: convolution,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.
For example, a block initialized with <code class="docutils literal notranslate"><span class="pre">order='CNA'</span></code> will
do convolution first, then normalization, then nonlinearity.</p></li>
<li><p><strong>multi_channel</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, use
different masks for different channels.</p></li>
<li><p><strong>return_mask</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the
forward call also returns a new mask.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.PartialConv3dBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.PartialConv3dBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.Res1dBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.</code><code class="sig-name descname">Res1dBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size=3</em>, <em class="sig-param">padding=1</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">padding_mode='zeros'</em>, <em class="sig-param">weight_norm_type='none'</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type='none'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">skip_activation_norm=True</em>, <em class="sig-param">skip_nonlinearity=False</em>, <em class="sig-param">nonlinearity='leakyrelu'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">hidden_channels_equal_out_channels=False</em>, <em class="sig-param">order='CNACNA'</em>, <em class="sig-param">learn_shortcut=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/residual.html#Res1dBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.Res1dBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.residual._BaseResBlock</span></code></p>
<p>Residual block for 1D input.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=3</em>) – Kernel size for the
convolutional filters in the residual link.</p></li>
<li><p><strong>padding</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Padding size.</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Dilation factor.</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Number of convolutional/linear
groups.</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em><em>, </em><em>default='zeros'</em>) – Type of padding:
<code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>skip_activation_norm</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies activation norm to the
learned shortcut connection.</p></li>
<li><p><strong>skip_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies nonlinearity to the
learned shortcut connection.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function in the residual link.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layers.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds
Gaussian noise with learnable magnitude to the convolution output.</p></li>
<li><p><strong>hidden_channels_equal_out_channels</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, set the hidden channel number to be equal to the
output channel number. If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the hidden channel number
equals to the smaller of the input channel number and the
output channel number.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNACNA'</em>) – Order of operations
in the residual link.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: convolution,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.</p></li>
<li><p><strong>learn_shortcut</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, always use
a convolutional shortcut instead of an identity one, otherwise only
use a convolutional one if input and output have different number of
channels.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.Res1dBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.Res1dBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.Res2dBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.</code><code class="sig-name descname">Res2dBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size=3</em>, <em class="sig-param">padding=1</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">padding_mode='zeros'</em>, <em class="sig-param">weight_norm_type='none'</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type='none'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">skip_activation_norm=True</em>, <em class="sig-param">skip_nonlinearity=False</em>, <em class="sig-param">nonlinearity='leakyrelu'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">hidden_channels_equal_out_channels=False</em>, <em class="sig-param">order='CNACNA'</em>, <em class="sig-param">learn_shortcut=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/residual.html#Res2dBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.Res2dBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.residual._BaseResBlock</span></code></p>
<p>Residual block for 2D input.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=3</em>) – Kernel size for the
convolutional filters in the residual link.</p></li>
<li><p><strong>padding</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Padding size.</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Dilation factor.</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Number of convolutional/linear
groups.</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em><em>, </em><em>default='zeros'</em>) – Type of padding:
<code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>skip_activation_norm</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies activation norm to the
learned shortcut connection.</p></li>
<li><p><strong>skip_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies nonlinearity to the
learned shortcut connection.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function in the residual link.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layers.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds
Gaussian noise with learnable magnitude to the convolution output.</p></li>
<li><p><strong>hidden_channels_equal_out_channels</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, set the hidden channel number to be equal to the
output channel number. If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the hidden channel number
equals to the smaller of the input channel number and the
output channel number.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNACNA'</em>) – Order of operations
in the residual link.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: convolution,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.</p></li>
<li><p><strong>learn_shortcut</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, always use
a convolutional shortcut instead of an identity one, otherwise only
use a convolutional one if input and output have different number of
channels.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.Res2dBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.Res2dBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.Res3dBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.</code><code class="sig-name descname">Res3dBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size=3</em>, <em class="sig-param">padding=1</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">padding_mode='zeros'</em>, <em class="sig-param">weight_norm_type='none'</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type='none'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">skip_activation_norm=True</em>, <em class="sig-param">skip_nonlinearity=False</em>, <em class="sig-param">nonlinearity='leakyrelu'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">hidden_channels_equal_out_channels=False</em>, <em class="sig-param">order='CNACNA'</em>, <em class="sig-param">learn_shortcut=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/residual.html#Res3dBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.Res3dBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.residual._BaseResBlock</span></code></p>
<p>Residual block for 3D input.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=3</em>) – Kernel size for the
convolutional filters in the residual link.</p></li>
<li><p><strong>padding</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Padding size.</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Dilation factor.</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Number of convolutional/linear
groups.</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em><em>, </em><em>default='zeros'</em>) – Type of padding:
<code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>skip_activation_norm</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies activation norm to the
learned shortcut connection.</p></li>
<li><p><strong>skip_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies nonlinearity to the
learned shortcut connection.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function in the residual link.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layers.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds
Gaussian noise with learnable magnitude to the convolution output.</p></li>
<li><p><strong>hidden_channels_equal_out_channels</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, set the hidden channel number to be equal to the
output channel number. If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the hidden channel number
equals to the smaller of the input channel number and the
output channel number.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNACNA'</em>) – Order of operations
in the residual link.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: convolution,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.</p></li>
<li><p><strong>learn_shortcut</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, always use
a convolutional shortcut instead of an identity one, otherwise only
use a convolutional one if input and output have different number of
channels.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.Res3dBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.Res3dBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.UpRes2dBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.</code><code class="sig-name descname">UpRes2dBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size=3</em>, <em class="sig-param">padding=1</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">padding_mode='zeros'</em>, <em class="sig-param">weight_norm_type='none'</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type='none'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">skip_activation_norm=True</em>, <em class="sig-param">skip_nonlinearity=False</em>, <em class="sig-param">nonlinearity='leakyrelu'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">hidden_channels_equal_out_channels=False</em>, <em class="sig-param">order='CNACNA'</em>, <em class="sig-param">upsample=&lt;class 'torch.nn.modules.upsampling.Upsample'&gt;</em>, <em class="sig-param">up_factor=2</em>, <em class="sig-param">learn_shortcut=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/residual.html#UpRes2dBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.UpRes2dBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.residual._BaseUpResBlock</span></code></p>
<p>Residual block for 2D input with downsampling.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=3</em>) – Kernel size for the
convolutional filters in the residual link.</p></li>
<li><p><strong>padding</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Padding size.</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Dilation factor.</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Number of convolutional/linear
groups.</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em><em>, </em><em>default='zeros'</em>) – Type of padding:
<code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>skip_activation_norm</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies activation norm to the
learned shortcut connection.</p></li>
<li><p><strong>skip_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies nonlinearity to the
learned shortcut connection.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function in the residual link.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layers.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds
Gaussian noise with learnable magnitude to the convolution output.</p></li>
<li><p><strong>hidden_channels_equal_out_channels</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, set the hidden channel number to be equal to the
output channel number. If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the hidden channel number
equals to the smaller of the input channel number and the
output channel number.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNACNA'</em>) – Order of operations
in the residual link.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: convolution,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.</p></li>
<li><p><strong>upsample</strong> (<em>class</em><em>, </em><em>optional</em><em>, </em><em>default=NearestUpsample</em>) – PPytorch
upsampling layer to be used.</p></li>
<li><p><strong>up_factor</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=2</em>) – Upsampling factor.</p></li>
<li><p><strong>learn_shortcut</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, always use
a convolutional shortcut instead of an identity one, otherwise only
use a convolutional one if input and output have different number of
channels.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.UpRes2dBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.UpRes2dBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.DownRes2dBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.</code><code class="sig-name descname">DownRes2dBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size=3</em>, <em class="sig-param">padding=1</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">padding_mode='zeros'</em>, <em class="sig-param">weight_norm_type='none'</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type='none'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">skip_activation_norm=True</em>, <em class="sig-param">skip_nonlinearity=False</em>, <em class="sig-param">nonlinearity='leakyrelu'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">hidden_channels_equal_out_channels=False</em>, <em class="sig-param">order='CNACNA'</em>, <em class="sig-param">pooling=&lt;class 'torch.nn.modules.pooling.AvgPool2d'&gt;</em>, <em class="sig-param">down_factor=2</em>, <em class="sig-param">learn_shortcut=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/residual.html#DownRes2dBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.DownRes2dBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.residual._BaseDownResBlock</span></code></p>
<p>Residual block for 2D input with downsampling.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=3</em>) – Kernel size for the
convolutional filters in the residual link.</p></li>
<li><p><strong>padding</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Padding size.</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Dilation factor.</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Number of convolutional/linear
groups.</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em><em>, </em><em>default='zeros'</em>) – Type of padding:
<code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>skip_activation_norm</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies activation norm to the
learned shortcut connection.</p></li>
<li><p><strong>skip_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies nonlinearity to the
learned shortcut connection.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function in the residual link.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layers.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds
Gaussian noise with learnable magnitude to the convolution output.</p></li>
<li><p><strong>hidden_channels_equal_out_channels</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, set the hidden channel number to be equal to the
output channel number. If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the hidden channel number
equals to the smaller of the input channel number and the
output channel number.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNACNA'</em>) – Order of operations
in the residual link.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: convolution,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.</p></li>
<li><p><strong>pooling</strong> (<em>class</em><em>, </em><em>optional</em><em>, </em><em>default=nn.AvgPool2d</em>) – Pytorch pooling
layer to be used.</p></li>
<li><p><strong>down_factor</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=2</em>) – Downsampling factor.</p></li>
<li><p><strong>learn_shortcut</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, always use
a convolutional shortcut instead of an identity one, otherwise only
use a convolutional one if input and output have different number of
channels.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.DownRes2dBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.DownRes2dBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.ResLinearBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.</code><code class="sig-name descname">ResLinearBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">weight_norm_type='none'</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type='none'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">skip_activation_norm=True</em>, <em class="sig-param">skip_nonlinearity=False</em>, <em class="sig-param">nonlinearity='leakyrelu'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">hidden_channels_equal_out_channels=False</em>, <em class="sig-param">order='CNACNA'</em>, <em class="sig-param">learn_shortcut=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/residual.html#ResLinearBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.ResLinearBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.residual._BaseResBlock</span></code></p>
<p>Residual block with full-connected layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>skip_activation_norm</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies activation norm to the
learned shortcut connection.</p></li>
<li><p><strong>skip_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies nonlinearity to the
learned shortcut connection.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function in the residual link.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layers.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, add
Gaussian noise with learnable magnitude after the
fully-connected layer.</p></li>
<li><p><strong>hidden_channels_equal_out_channels</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, set the hidden channel number to be equal to the
output channel number. If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the hidden channel number
equals to the smaller of the input channel number and the
output channel number.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNACNA'</em>) – Order of operations
in the residual link.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: fully-connected,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.</p></li>
<li><p><strong>learn_shortcut</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, always use
a convolutional shortcut instead of an identity one, otherwise only
use a convolutional one if input and output have different number of
channels.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.ResLinearBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.ResLinearBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.HyperRes2dBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.</code><code class="sig-name descname">HyperRes2dBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size=3</em>, <em class="sig-param">padding=1</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">padding_mode='zeros'</em>, <em class="sig-param">weight_norm_type=''</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type=''</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">skip_activation_norm=True</em>, <em class="sig-param">skip_nonlinearity=False</em>, <em class="sig-param">nonlinearity='leakyrelu'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">hidden_channels_equal_out_channels=False</em>, <em class="sig-param">order='CNACNA'</em>, <em class="sig-param">is_hyper_conv=False</em>, <em class="sig-param">is_hyper_norm=False</em>, <em class="sig-param">learn_shortcut=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/residual.html#HyperRes2dBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.HyperRes2dBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.residual._BaseHyperResBlock</span></code></p>
<p>Hyper residual block for 2D input.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=3</em>) – Kernel size for the
convolutional filters in the residual link.</p></li>
<li><p><strong>padding</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Padding size.</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Dilation factor.</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Number of convolutional/linear
groups.</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em><em>, </em><em>default='zeros'</em>) – Type of padding:
<code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>skip_activation_norm</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies activation norm to the
learned shortcut connection.</p></li>
<li><p><strong>skip_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies nonlinearity to the
learned shortcut connection.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function in the residual link.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layers.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds
Gaussian noise with learnable magnitude to the convolution output.</p></li>
<li><p><strong>hidden_channels_equal_out_channels</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, set the hidden channel number to be equal to the
output channel number. If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the hidden channel number
equals to the smaller of the input channel number and the
output channel number.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNACNA'</em>) – Order of operations
in the residual link.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: convolution,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.</p></li>
<li><p><strong>is_hyper_conv</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, use
<code class="docutils literal notranslate"><span class="pre">HyperConv2d</span></code>, otherwise use <code class="docutils literal notranslate"><span class="pre">torch.nn.Conv2d</span></code>.</p></li>
<li><p><strong>is_hyper_norm</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, use
hyper normalizations.</p></li>
<li><p><strong>learn_shortcut</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, always use
a convolutional shortcut instead of an identity one, otherwise only
use a convolutional one if input and output have different number of
channels.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.HyperRes2dBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.HyperRes2dBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.MultiOutRes2dBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.</code><code class="sig-name descname">MultiOutRes2dBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size=3</em>, <em class="sig-param">padding=1</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">padding_mode='zeros'</em>, <em class="sig-param">weight_norm_type='none'</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type='none'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">skip_activation_norm=True</em>, <em class="sig-param">skip_nonlinearity=False</em>, <em class="sig-param">nonlinearity='leakyrelu'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">hidden_channels_equal_out_channels=False</em>, <em class="sig-param">order='CNACNA'</em>, <em class="sig-param">learn_shortcut=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/residual.html#MultiOutRes2dBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.MultiOutRes2dBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.residual._BaseMultiOutResBlock</span></code></p>
<p>Residual block for 2D input. It can return multiple outputs, if some
layers in the block return more than one output.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=3</em>) – Kernel size for the
convolutional filters in the residual link.</p></li>
<li><p><strong>padding</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Padding size.</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Dilation factor.</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Number of convolutional/linear
groups.</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em><em>, </em><em>default='zeros'</em>) – Type of padding:
<code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>skip_activation_norm</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies activation norm to the
learned shortcut connection.</p></li>
<li><p><strong>skip_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies nonlinearity to the
learned shortcut connection.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function in the residual link.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layers.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds
Gaussian noise with learnable magnitude to the convolution output.</p></li>
<li><p><strong>hidden_channels_equal_out_channels</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, set the hidden channel number to be equal to the
output channel number. If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the hidden channel number
equals to the smaller of the input channel number and the
output channel number.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNACNA'</em>) – Order of operations
in the residual link.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: convolution,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.</p></li>
<li><p><strong>learn_shortcut</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, always use
a convolutional shortcut instead of an identity one, otherwise only
use a convolutional one if input and output have different number of
channels.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.MultiOutRes2dBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.MultiOutRes2dBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.PartialRes2dBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.</code><code class="sig-name descname">PartialRes2dBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size=3</em>, <em class="sig-param">padding=1</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">padding_mode='zeros'</em>, <em class="sig-param">weight_norm_type='none'</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type='none'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">skip_activation_norm=True</em>, <em class="sig-param">skip_nonlinearity=False</em>, <em class="sig-param">nonlinearity='leakyrelu'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">multi_channel=False</em>, <em class="sig-param">return_mask=True</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">hidden_channels_equal_out_channels=False</em>, <em class="sig-param">order='CNACNA'</em>, <em class="sig-param">learn_shortcut=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/residual.html#PartialRes2dBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.PartialRes2dBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.residual._BasePartialResBlock</span></code></p>
<p>Residual block for 2D input with partial convolution.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=3</em>) – Kernel size for the
convolutional filters in the residual link.</p></li>
<li><p><strong>padding</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Padding size.</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Dilation factor.</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Number of convolutional/linear
groups.</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em><em>, </em><em>default='zeros'</em>) – Type of padding:
<code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>skip_activation_norm</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies activation norm to the
learned shortcut connection.</p></li>
<li><p><strong>skip_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies nonlinearity to the
learned shortcut connection.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function in the residual link.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layers.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds
Gaussian noise with learnable magnitude to the convolution output.</p></li>
<li><p><strong>hidden_channels_equal_out_channels</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, set the hidden channel number to be equal to the
output channel number. If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the hidden channel number
equals to the smaller of the input channel number and the
output channel number.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNACNA'</em>) – Order of operations
in the residual link.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: convolution,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.</p></li>
<li><p><strong>learn_shortcut</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, always use
a convolutional shortcut instead of an identity one, otherwise only
use a convolutional one if input and output have different number of
channels.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.PartialRes2dBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.PartialRes2dBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.PartialRes3dBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.</code><code class="sig-name descname">PartialRes3dBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size=3</em>, <em class="sig-param">padding=1</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">padding_mode='zeros'</em>, <em class="sig-param">weight_norm_type='none'</em>, <em class="sig-param">weight_norm_params=None</em>, <em class="sig-param">activation_norm_type='none'</em>, <em class="sig-param">activation_norm_params=None</em>, <em class="sig-param">skip_activation_norm=True</em>, <em class="sig-param">skip_nonlinearity=False</em>, <em class="sig-param">nonlinearity='leakyrelu'</em>, <em class="sig-param">inplace_nonlinearity=False</em>, <em class="sig-param">multi_channel=False</em>, <em class="sig-param">return_mask=True</em>, <em class="sig-param">apply_noise=False</em>, <em class="sig-param">hidden_channels_equal_out_channels=False</em>, <em class="sig-param">order='CNACNA'</em>, <em class="sig-param">learn_shortcut=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/residual.html#PartialRes3dBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.PartialRes3dBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">imaginaire.layers.residual._BasePartialResBlock</span></code></p>
<p>Residual block for 3D input with partial convolution.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels in the output tensor.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=3</em>) – Kernel size for the
convolutional filters in the residual link.</p></li>
<li><p><strong>padding</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Padding size.</p></li>
<li><p><strong>dilation</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Dilation factor.</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Number of convolutional/linear
groups.</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em><em>, </em><em>default='zeros'</em>) – Type of padding:
<code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
<li><p><strong>weight_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of weight normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing weight normalization.</p></li>
<li><p><strong>activation_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of activation normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'instance'</span></code>, <code class="docutils literal notranslate"><span class="pre">'batch'</span></code>, <code class="docutils literal notranslate"><span class="pre">'sync_batch'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'layer_2d'</span></code>, <code class="docutils literal notranslate"><span class="pre">'group'</span></code>, <code class="docutils literal notranslate"><span class="pre">'adaptive'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'spatially_adaptive'</span></code> or <code class="docutils literal notranslate"><span class="pre">'hyper_spatially_adaptive'</span></code>.</p></li>
<li><p><strong>activation_norm_params</strong> (<em>obj</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – Parameters of activation normalization.
If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">activation_norm_params.__dict__</span></code> will be used as
keyword arguments when initializing activation normalization.</p></li>
<li><p><strong>skip_activation_norm</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies activation norm to the
learned shortcut connection.</p></li>
<li><p><strong>skip_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and
<code class="docutils literal notranslate"><span class="pre">learn_shortcut</span></code> is also <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies nonlinearity to the
learned shortcut connection.</p></li>
<li><p><strong>nonlinearity</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of nonlinear activation function in the residual link.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'leakyrelu'</span></code>, <code class="docutils literal notranslate"><span class="pre">'prelu'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> , <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code> or <code class="docutils literal notranslate"><span class="pre">'softmax'</span></code>.</p></li>
<li><p><strong>inplace_nonlinearity</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when initializing the nonlinearity layers.</p></li>
<li><p><strong>apply_noise</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds
Gaussian noise with learnable magnitude to the convolution output.</p></li>
<li><p><strong>hidden_channels_equal_out_channels</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, set the hidden channel number to be equal to the
output channel number. If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the hidden channel number
equals to the smaller of the input channel number and the
output channel number.</p></li>
<li><p><strong>order</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='CNACNA'</em>) – Order of operations
in the residual link.
<code class="docutils literal notranslate"><span class="pre">'C'</span></code>: convolution,
<code class="docutils literal notranslate"><span class="pre">'N'</span></code>: normalization,
<code class="docutils literal notranslate"><span class="pre">'A'</span></code>: nonlinear activation.</p></li>
<li><p><strong>learn_shortcut</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, always use
a convolutional shortcut instead of an identity one, otherwise only
use a convolutional one if input and output have different number of
channels.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="imaginaire.layers.PartialRes3dBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.PartialRes3dBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="imaginaire.layers.NonLocal2dBlock">
<em class="property">class </em><code class="sig-prename descclassname">imaginaire.layers.</code><code class="sig-name descname">NonLocal2dBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">scale=True</em>, <em class="sig-param">clamp=False</em>, <em class="sig-param">weight_norm_type='none'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/non_local.html#NonLocal2dBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.NonLocal2dBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Self attention Layer</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input tensor.</p></li>
<li><p><strong>scale</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, scale the
output by a learnable parameter.</p></li>
<li><p><strong>clamp</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=``False``</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, clamp the
scaling parameter to (-1, 1).</p></li>
<li><p><strong>weight_norm_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default='none'</em>) – Type of weight normalization.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'spectral'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>
or <code class="docutils literal notranslate"><span class="pre">'weight_demod'</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="imaginaire.layers.NonLocal2dBlock.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/imaginaire/layers/non_local.html#NonLocal2dBlock.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#imaginaire.layers.NonLocal2dBlock.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>tensor</em>) – input feature maps (B X C X W X H)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p>out (tensor) : self attention value + input feature</p></li>
<li><p>attention (tensor): B x N x N (N is Width*Height)</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(tuple)</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="imaginaire.layers.NonLocal2dBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#imaginaire.layers.NonLocal2dBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="imaginaire.losses.html" class="btn btn-neutral float-right" title="imaginaire.losses package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="imaginaire.generators.html" class="btn btn-neutral float-left" title="imaginaire.generators package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright NVIDIA

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>